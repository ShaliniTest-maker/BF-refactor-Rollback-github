# tox.ini - Performance Testing Configuration for Flask Migration Validation
# 
# This configuration file orchestrates comprehensive multi-environment performance testing
# for the Flask 3.1.1 application migration from Node.js/Express.js, utilizing tox 4.26.0
# for environment isolation and pytest-benchmark 5.1.0 for statistical performance measurement.
#
# Key Features:
# - Python 3.13.3 primary environment with Flask 3.1.1 compatibility
# - Multi-environment testing orchestration with parallel execution
# - Baseline comparison testing against Node.js system performance
# - Comprehensive performance metrics collection and reporting
# - Integration with OpenTelemetry monitoring and observability
#
# Technical Specification Compliance:
# - Section 4.7.1: pytest-benchmark 5.1.0 performance measurement integration
# - Section 4.7.2: tox 4.26.0 multi-environment testing orchestration
# - Section 4.11.1: Performance SLA validation (sub-200ms API, sub-100ms DB)
# - Section 4.11.3: Python 3.13.3 and Flask 3.1.1 compatibility validation
# - Section 5.1.1: Flask application factory pattern testing configuration
# - Section 6.5.1.1: Comprehensive test reporting and performance metrics

[tox]
# tox 4.26.0 minimum version requirement for multi-environment orchestration
# and enhanced parallel execution capabilities with improved dependency management
minversion = 4.26.0

# Environment list for comprehensive performance testing coverage
# Each environment targets specific performance validation aspects
envlist = 
    py313,                    # Primary Python 3.13.3 environment
    performance,              # Main performance testing with pytest-benchmark
    baseline-comparison,      # Node.js baseline comparison testing
    memory-profiling,         # Memory usage and GC performance analysis
    concurrent-load,          # Concurrent user load testing scenarios
    database-benchmarks,      # SQLAlchemy query performance validation
    api-benchmarks,           # Flask API response time benchmarks
    integration-performance,  # Integration testing with performance validation
    reporting                 # Comprehensive test reporting and metrics collection

# Parallel execution configuration for efficient multi-environment testing
# Enables isolated virtual environment management with concurrent execution
parallel_show_output = true
skipsdist = false

# Base Python version configuration for all environments
# Python 3.13.3 requirement per Section 4.11.3 for Flask 3.1.1 compatibility
basepython = python3.13

[testenv]
# Default test environment configuration
# Establishes foundation for all performance testing environments
description = Base environment for Flask performance testing with Python 3.13.3

# Core dependencies for all testing environments
# Flask 3.1.1 compatibility with comprehensive performance testing tools
deps =
    # Core Flask framework and extensions
    Flask==3.1.1
    Flask-SQLAlchemy==3.1.1
    Flask-Migrate==4.1.0
    
    # Performance testing and benchmarking tools
    pytest>=8.0.0
    pytest-benchmark==5.1.0
    pytest-flask==1.3.0
    pytest-cov>=4.0.0
    pytest-xdist>=3.0.0
    
    # Memory profiling and analysis tools
    memory-profiler>=0.61.0
    pympler>=0.9
    psutil>=5.9.0
    
    # OpenTelemetry integration for monitoring
    opentelemetry-api>=1.20.0
    opentelemetry-sdk>=1.20.0
    opentelemetry-instrumentation-flask>=0.41b0
    opentelemetry-instrumentation-sqlalchemy>=0.41b0
    
    # Load testing and concurrent execution utilities
    locust>=2.17.0
    asyncio-throttle>=1.0.2
    threading-utils>=0.3
    
    # Monitoring and metrics collection
    prometheus-flask-exporter>=0.23.0
    structlog>=23.2.0
    
    # Database and service integration testing
    requests>=2.31.0
    httpx>=0.25.0
    
    # Development and debugging tools
    werkzeug>=3.0.0
    click>=8.1.0

# Flask application factory environment variables
# Configuration for comprehensive Flask application testing per Section 5.1.1
setenv =
    FLASK_APP = src.app:create_app
    FLASK_ENV = testing
    FLASK_DEBUG = 0
    
    # Database configuration for testing
    DATABASE_URL = sqlite:///:memory:
    SQLALCHEMY_DATABASE_URI = sqlite:///:memory:
    SQLALCHEMY_TRACK_MODIFICATIONS = false
    
    # Testing-specific configuration
    TESTING = true
    WTF_CSRF_ENABLED = false
    SECRET_KEY = testing-secret-key-for-performance-validation
    
    # Performance testing configuration
    PYTEST_BENCHMARK_DISABLE_GC = 1
    PYTEST_BENCHMARK_SKIP_SLOW = 0
    PYTEST_BENCHMARK_WARMUP = true
    PYTEST_BENCHMARK_WARMUP_ITERATIONS = 5
    
    # OpenTelemetry configuration for performance monitoring
    OTEL_SDK_DISABLED = false
    OTEL_PYTHON_LOG_CORRELATION = true
    OTEL_PYTHON_LOG_FORMAT = "%(msg)s [trace_id=%(otelTraceID)s span_id=%(otelSpanID)s]"
    
    # Monitoring and observability settings
    PROMETHEUS_METRICS_ENABLED = true
    PERFORMANCE_MONITORING_ENABLED = true
    
    # Migration validation settings
    BASELINE_COMPARISON_ENABLED = true
    MIGRATION_VALIDATION_MODE = true

# Base commands configuration
# Default test execution pattern for performance validation
commands = 
    python -m pytest tests/performance/ -v --tb=short

# Change directory to project root for consistent test execution
changedir = {toxinidir}/../..

# Allow external network access for baseline comparison testing
allowlist_externals = 
    echo
    curl
    node
    npm

[testenv:py313]
# Primary Python 3.13.3 environment for Flask 3.1.1 compatibility validation
# Core environment for all performance testing scenarios per Section 4.11.3
description = Primary Python 3.13.3 environment with Flask 3.1.1 compatibility validation

commands =
    echo "=== Python 3.13.3 Flask 3.1.1 Compatibility Validation ==="
    python --version
    python -c "import flask; print(f'Flask version: {flask.__version__}')"
    python -c "import sys; print(f'Python path: {sys.executable}')"
    
    # Run basic Flask application factory validation
    python -m pytest tests/performance/conftest.py -v --tb=short
    
    # Execute comprehensive performance test suite
    python -m pytest tests/performance/ -v --tb=short --benchmark-skip

[testenv:performance]
# Main performance testing environment with pytest-benchmark 5.1.0 integration
# Comprehensive performance measurement and validation per Section 4.7.1
description = Primary performance testing with pytest-benchmark 5.1.0 statistical measurement

# Additional performance-specific dependencies
deps =
    {[testenv]deps}
    pytest-benchmark[histogram]==5.1.0
    matplotlib>=3.7.0
    seaborn>=0.12.0

# Performance testing specific environment variables
setenv =
    {[testenv]setenv}
    PYTEST_BENCHMARK_AUTOSAVE = true
    PYTEST_BENCHMARK_STORAGE = file://{toxinidir}/benchmark_results
    PYTEST_BENCHMARK_HISTOGRAM = true
    PYTEST_BENCHMARK_COMPARE_FAIL = mean:10%
    PYTEST_BENCHMARK_SORT = mean

commands =
    echo "=== Flask Performance Testing with pytest-benchmark 5.1.0 ==="
    
    # API response time benchmarks - Sub-200ms SLA validation per Section 4.11.1
    python -m pytest tests/performance/test_api_benchmarks.py -v \
        --benchmark-only \
        --benchmark-warmup=on \
        --benchmark-sort=mean \
        --benchmark-autosave \
        --benchmark-histogram={toxinidir}/benchmark_results/api_histogram.svg
    
    # Database query performance - Sub-100ms SLA validation per Section 4.11.1  
    python -m pytest tests/performance/test_database_benchmarks.py -v \
        --benchmark-only \
        --benchmark-warmup=on \
        --benchmark-sort=mean \
        --benchmark-autosave \
        --benchmark-histogram={toxinidir}/benchmark_results/db_histogram.svg
    
    # Comprehensive performance test suite execution
    python -m pytest tests/performance/ -v \
        --benchmark-only \
        --benchmark-compare-fail=mean:10% \
        --benchmark-json={toxinidir}/benchmark_results/performance_results.json

[testenv:baseline-comparison]
# Node.js baseline comparison testing environment per Section 4.7.2
# Real-time Flask vs Node.js performance validation and parity verification
description = Flask vs Node.js baseline comparison with real-time performance validation

# Baseline comparison specific dependencies
deps =
    {[testenv]deps}
    deepdiff>=6.7.0
    numpy>=1.25.0
    scipy>=1.11.0

# Baseline comparison environment configuration
setenv =
    {[testenv]setenv}
    BASELINE_COMPARISON_MODE = true
    NODEJS_BASELINE_ENDPOINT = http://localhost:3000
    FLASK_TEST_ENDPOINT = http://localhost:5000
    PERFORMANCE_TOLERANCE_PERCENT = 10
    FUNCTIONAL_PARITY_REQUIRED = true

commands =
    echo "=== Flask vs Node.js Baseline Comparison Testing ==="
    
    # Functional parity validation - 100% equivalence requirement
    python -m pytest tests/performance/test_baseline_comparison.py::test_functional_parity -v \
        --tb=short \
        --benchmark-skip
    
    # Performance comparison validation
    python -m pytest tests/performance/test_baseline_comparison.py::test_performance_comparison -v \
        --benchmark-only \
        --benchmark-compare-fail=mean:10%
    
    # Comprehensive baseline comparison suite
    python -m pytest tests/performance/test_baseline_comparison.py -v \
        --benchmark-autosave \
        --benchmark-json={toxinidir}/benchmark_results/baseline_comparison.json

[testenv:memory-profiling]
# Memory usage profiling and optimization validation per Section 6.5.1.1
# Python GC performance monitoring and memory footprint analysis
description = Memory profiling with Python GC performance monitoring and leak detection

# Memory profiling specific dependencies
deps =
    {[testenv]deps}
    memory-profiler>=0.61.0
    pympler>=0.9
    tracemalloc-tools>=0.2.0
    guppy3>=3.1.3

# Memory profiling environment configuration
setenv =
    {[testenv]setenv}
    PYTHONTRACEMALLOC = 1
    MEMORY_PROFILING_ENABLED = true
    GC_MONITORING_ENABLED = true
    MEMORY_LEAK_DETECTION = true

commands =
    echo "=== Memory Profiling and GC Performance Analysis ==="
    
    # Memory allocation pattern analysis
    python -m pytest tests/performance/test_memory_profiling.py::test_memory_allocation -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Python GC pause time monitoring per Section 6.5.1.1
    python -m pytest tests/performance/test_memory_profiling.py::test_gc_performance -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Memory leak detection with long-running scenarios
    python -m pytest tests/performance/test_memory_profiling.py::test_memory_leak_detection -v \
        --tb=short \
        --benchmark-skip
    
    # Comprehensive memory profiling suite
    python -m pytest tests/performance/test_memory_profiling.py -v \
        --benchmark-autosave \
        --benchmark-json={toxinidir}/benchmark_results/memory_profiling.json

[testenv:concurrent-load]
# Concurrent user load testing with threading framework validation
# Multi-user scenario simulation and throughput analysis per Section 4.7.1
description = Concurrent user load testing with threading and capacity validation

# Concurrent load testing specific dependencies
deps =
    {[testenv]deps}
    locust>=2.17.0
    gevent>=23.9.0
    asyncio-throttle>=1.0.2

# Concurrent load testing environment configuration
setenv =
    {[testenv]setenv}
    CONCURRENT_USERS = 100
    LOAD_TEST_DURATION = 300
    RAMP_UP_TIME = 60
    THREAD_POOL_SIZE = 50

commands =
    echo "=== Concurrent User Load Testing and Capacity Validation ==="
    
    # Concurrent user simulation framework
    python -m pytest tests/performance/test_concurrent_load.py::test_concurrent_users -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Thread pool utilization monitoring
    python -m pytest tests/performance/test_concurrent_load.py::test_thread_pool_performance -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # System throughput validation
    python -m pytest tests/performance/test_concurrent_load.py::test_system_throughput -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Comprehensive concurrent load testing suite
    python -m pytest tests/performance/test_concurrent_load.py -v \
        --benchmark-autosave \
        --benchmark-json={toxinidir}/benchmark_results/concurrent_load.json

[testenv:database-benchmarks]
# SQLAlchemy query performance benchmarking per Section 4.11.1
# Sub-100ms query response time validation and connection pool efficiency
description = SQLAlchemy query performance with sub-100ms response time validation

# Database benchmarking specific dependencies
deps =
    {[testenv]deps}
    sqlalchemy-utils>=0.41.0
    factory-boy>=3.3.0
    faker>=20.1.0

# Database benchmarking environment configuration
setenv =
    {[testenv]setenv}
    DATABASE_QUERY_TIMEOUT = 100
    CONNECTION_POOL_SIZE = 10
    CONNECTION_POOL_OVERFLOW = 20
    SQLALCHEMY_ECHO = false
    QUERY_PERFORMANCE_MONITORING = true

commands =
    echo "=== SQLAlchemy Query Performance Benchmarking ==="
    
    # Basic CRUD operation performance validation
    python -m pytest tests/performance/test_database_benchmarks.py::test_crud_performance -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Complex query performance with relationship loading
    python -m pytest tests/performance/test_database_benchmarks.py::test_relationship_loading -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Connection pool utilization efficiency
    python -m pytest tests/performance/test_database_benchmarks.py::test_connection_pool -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Comprehensive database performance suite
    python -m pytest tests/performance/test_database_benchmarks.py -v \
        --benchmark-autosave \
        --benchmark-json={toxinidir}/benchmark_results/database_benchmarks.json

[testenv:api-benchmarks]
# Flask API response time benchmarking per Section 4.11.1
# Sub-200ms API response time SLA validation and endpoint performance analysis
description = Flask API response time benchmarking with sub-200ms SLA validation

# API benchmarking specific dependencies
deps =
    {[testenv]deps}
    httpx[http2]>=0.25.0
    aiohttp>=3.9.0
    requests-cache>=1.1.0

# API benchmarking environment configuration
setenv =
    {[testenv]setenv}
    API_RESPONSE_TIMEOUT = 200
    API_PERFORMANCE_MONITORING = true
    ENDPOINT_COVERAGE_REQUIRED = true
    BLUEPRINT_PERFORMANCE_TRACKING = true

commands =
    echo "=== Flask API Response Time Benchmarking ==="
    
    # Individual endpoint performance validation
    python -m pytest tests/performance/test_api_benchmarks.py::test_endpoint_performance -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Blueprint route performance analysis
    python -m pytest tests/performance/test_api_benchmarks.py::test_blueprint_performance -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Authentication endpoint performance per Section 4.11.1 (sub-150ms)
    python -m pytest tests/performance/test_api_benchmarks.py::test_auth_performance -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Comprehensive API performance suite
    python -m pytest tests/performance/test_api_benchmarks.py -v \
        --benchmark-autosave \
        --benchmark-json={toxinidir}/benchmark_results/api_benchmarks.json

[testenv:integration-performance]
# Integration performance testing with comprehensive system validation
# End-to-end performance testing across all application components
description = Integration performance testing with end-to-end system validation

# Integration testing specific dependencies
deps =
    {[testenv]deps}
    pytest-integration>=0.2.0
    docker>=6.1.0
    testcontainers>=3.7.0

# Integration performance environment configuration
setenv =
    {[testenv]setenv}
    INTEGRATION_TEST_MODE = true
    SYSTEM_INTEGRATION_REQUIRED = true
    END_TO_END_PERFORMANCE = true
    COMPONENT_INTEGRATION_TESTING = true

commands =
    echo "=== Integration Performance Testing ==="
    
    # System integration performance validation
    python -m pytest tests/integration/performance/ -v \
        --benchmark-only \
        --benchmark-sort=mean
    
    # Cross-component performance testing
    python -m pytest tests/performance/ tests/integration/performance/ -v \
        --benchmark-autosave \
        --benchmark-json={toxinidir}/benchmark_results/integration_performance.json

[testenv:reporting]
# Comprehensive test reporting and performance metrics collection per Section 6.5.1.1
# Aggregated performance analysis and migration validation reporting
description = Comprehensive performance test reporting and metrics aggregation

# Reporting specific dependencies
deps =
    {[testenv]deps}
    jinja2>=3.1.0
    plotly>=5.17.0
    pandas>=2.1.0
    openpyxl>=3.1.0

# Reporting environment configuration
setenv =
    {[testenv]setenv}
    REPORT_GENERATION_ENABLED = true
    METRICS_AGGREGATION_ENABLED = true
    DASHBOARD_GENERATION = true
    MIGRATION_VALIDATION_REPORT = true

commands =
    echo "=== Performance Test Reporting and Metrics Collection ==="
    
    # Generate comprehensive performance report
    python -c "
    import json
    import os
    from pathlib import Path
    
    # Aggregate benchmark results from all environments
    results_dir = Path('{toxinidir}/benchmark_results')
    if results_dir.exists():
        print('=== Performance Test Results Summary ===')
        for result_file in results_dir.glob('*.json'):
            print(f'Report: {{result_file.name}}')
            try:
                with open(result_file) as f:
                    data = json.load(f)
                    if 'benchmarks' in data:
                        print(f'  Benchmarks: {{len(data[\"benchmarks\"])}}')
                        for bench in data['benchmarks'][:3]:  # Show first 3
                            print(f'    {{bench[\"name\"]}}: {{bench[\"stats\"][\"mean\"]:.4f}}s')
            except Exception as e:
                print(f'  Error reading {{result_file}}: {{e}}')
        print('=== End Performance Summary ===')
    else:
        print('No benchmark results found. Run performance environments first.')
    "
    
    # Validation summary report
    echo "=== Migration Validation Summary ==="
    echo "Performance testing completed for Flask 3.1.1 migration validation"
    echo "Key SLA Requirements:"
    echo "  - API Response Time: < 200ms (Section 4.11.1)"
    echo "  - Database Query Time: < 100ms (Section 4.11.1)" 
    echo "  - Authentication Response: < 150ms (Section 4.11.1)"
    echo "  - Functional Parity: 100% (Section 4.7.1)"
    echo "Review benchmark results in: {toxinidir}/benchmark_results/"

[testenv:parallel]
# Parallel execution orchestration for efficient multi-environment testing
# Coordinated parallel test execution across all performance environments
description = Parallel execution orchestration for comprehensive performance validation

deps = {[testenv]deps}

# Parallel execution environment configuration
setenv =
    {[testenv]setenv}
    PARALLEL_EXECUTION_ENABLED = true
    MAX_PARALLEL_ENVIRONMENTS = 4
    ENVIRONMENT_ISOLATION = true

commands =
    echo "=== Parallel Performance Testing Orchestration ==="
    
    # Execute core performance environments in parallel
    python -c "
    import subprocess
    import concurrent.futures
    import sys
    
    # Define core performance test environments
    environments = [
        'performance',
        'baseline-comparison', 
        'memory-profiling',
        'concurrent-load',
        'database-benchmarks',
        'api-benchmarks'
    ]
    
    def run_tox_env(env):
        '''Execute tox environment and return results'''
        try:
            result = subprocess.run(
                ['tox', '-e', env], 
                cwd='{toxinidir}',
                capture_output=True, 
                text=True,
                timeout=1800  # 30 minute timeout per environment
            )
            return (env, result.returncode, result.stdout, result.stderr)
        except subprocess.TimeoutExpired:
            return (env, -1, '', f'Environment {{env}} timed out after 30 minutes')
        except Exception as e:
            return (env, -2, '', f'Environment {{env}} failed with error: {{str(e)}}')
    
    print('Starting parallel execution of performance test environments...')
    
    # Execute environments in parallel with thread pool
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        future_to_env = {{executor.submit(run_tox_env, env): env for env in environments}}
        
        results = []
        for future in concurrent.futures.as_completed(future_to_env):
            env = future_to_env[future]
            try:
                result = future.result()
                results.append(result)
                env_name, return_code, stdout, stderr = result
                
                if return_code == 0:
                    print(f'✓ Environment {{env_name}} completed successfully')
                else:
                    print(f'✗ Environment {{env_name}} failed with code {{return_code}}')
                    if stderr:
                        print(f'  Error: {{stderr[:200]}}...')
                        
            except Exception as exc:
                print(f'✗ Environment {{env}} generated an exception: {{exc}}')
                results.append((env, -3, '', str(exc)))
    
    # Summary report
    print('\\n=== Parallel Execution Summary ===')
    success_count = sum(1 for _, code, _, _ in results if code == 0)
    total_count = len(results)
    
    print(f'Successful environments: {{success_count}}/{{total_count}}')
    
    for env_name, return_code, _, stderr in results:
        status = '✓ PASS' if return_code == 0 else '✗ FAIL'
        print(f'  {{env_name}}: {{status}}')
    
    # Exit with error if any environment failed
    if success_count < total_count:
        print('\\nSome performance test environments failed. Check logs above.')
        sys.exit(1)
    else:
        print('\\nAll performance test environments completed successfully!')
        sys.exit(0)
    "

# Global tox configuration for performance testing optimization
[gh-actions]
# GitHub Actions integration for CI/CD pipeline performance testing
python =
    3.13: py313, performance, baseline-comparison

[coverage:run]
# Code coverage configuration for performance testing
source = src/
omit = 
    tests/*
    */venv/*
    */migrations/*

[coverage:report]
# Coverage reporting configuration
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError

# Performance testing result storage configuration
[benchmark]
# pytest-benchmark configuration for result storage and comparison
storage = file://{toxinidir}/benchmark_results
histogram = true
sort = mean
compare-fail = mean:10%
autosave = true
warmup = true
warmup-iterations = 5