# tox configuration for multi-environment Flask performance testing orchestration
# Enables comprehensive performance validation across Python versions with pytest-benchmark integration
# Supports migration validation through baseline comparison and concurrent testing scenarios

[tox]
# tox 4.26.0 multi-environment testing orchestration configuration
minversion = 4.26.0
# Primary testing environments for comprehensive Flask performance validation
envlist = py313-flask311-performance, py313-baseline-comparison, py313-concurrent-load, py313-memory-profiling
# Enable parallel execution with environment isolation for efficient testing
parallel_show_output = true
# Skip missing interpreters to prevent test failures in CI/CD environments
skip_missing_interpreters = true
# Isolated build environment for dependency management
isolated_build = true

[testenv]
# Base environment configuration for all performance testing scenarios
# Python 3.13.3 with Flask 3.1.1 compatibility requirements
basepython = python3.13
# Virtual environment isolation with comprehensive dependency installation
deps = 
    # Core Flask framework dependencies
    Flask==3.1.1
    Werkzeug>=3.1
    ItsDangerous>=2.2
    Jinja2>=3.1.2
    Click>=8.1.3
    Blinker>=1.9
    
    # Database integration for performance testing
    Flask-SQLAlchemy==3.1.1
    Flask-Migrate==4.1.0
    
    # Performance testing and benchmarking core dependencies
    pytest-benchmark==5.1.0
    pytest-flask==1.3.0
    pytest>=8.0.0
    pytest-xdist>=3.5.0  # For parallel test execution
    pytest-html>=4.1.0   # For comprehensive HTML reporting
    pytest-cov>=4.0.0    # For coverage analysis during performance testing
    
    # Memory profiling and analysis dependencies
    memory-profiler>=0.61.0
    pympler>=0.9
    psutil>=5.9.0
    
    # Statistical analysis for performance validation
    numpy>=1.24.0
    pandas>=2.0.0
    scipy>=1.10.0
    
    # HTTP client for baseline comparison testing
    requests>=2.31.0
    httpx>=0.25.0
    
    # Additional testing utilities
    freezegun>=1.2.0     # For time-based testing
    factory-boy>=3.3.0   # For test data generation
    
    # Development dependencies from requirements-performance.txt
    -r requirements-performance.txt

# Environment variables for Flask application factory setup and performance testing
setenv =
    # Flask application configuration for testing
    FLASK_APP = src.app:create_app
    FLASK_ENV = testing
    FLASK_DEBUG = 0
    
    # Database configuration for performance testing
    DATABASE_URL = sqlite:///:memory:
    SQLALCHEMY_DATABASE_URI = sqlite:///:memory:
    SQLALCHEMY_TRACK_MODIFICATIONS = false
    SQLALCHEMY_ECHO = false
    
    # Performance testing configuration
    PERFORMANCE_BASELINE_URL = http://localhost:3000
    NODEJS_BASELINE_ENABLED = true
    BENCHMARK_STORAGE_PATH = {envtmpdir}/benchmark_results
    
    # Authentication configuration for testing
    SECRET_KEY = test-secret-key-for-performance-testing
    JWT_SECRET_KEY = test-jwt-secret-key
    AUTH0_DOMAIN = test.auth0.com
    AUTH0_CLIENT_ID = test-client-id
    AUTH0_CLIENT_SECRET = test-client-secret
    
    # Threading and concurrency configuration
    THREADING_MAX_WORKERS = 10
    CONCURRENT_USERS_MAX = 100
    
    # Monitoring and observability configuration
    OTEL_EXPORTER_OTLP_ENDPOINT = http://localhost:4317
    PROMETHEUS_METRICS_PORT = 9090
    MONITORING_ENABLED = false
    
    # Performance thresholds for validation
    API_RESPONSE_TIME_THRESHOLD_MS = 200
    DATABASE_QUERY_TIME_THRESHOLD_MS = 100
    AUTHENTICATION_TIME_THRESHOLD_MS = 150
    
# Test execution commands with comprehensive performance validation
commands = 
    # Display environment information for debugging
    python --version
    pip list
    
    # Execute performance test suite with pytest-benchmark integration
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH} \
        --benchmark-autosave \
        --benchmark-compare-fail=min:5% \
        --benchmark-compare-fail=mean:10% \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/benchmark.json \
        --html={envtmpdir}/performance_report.html \
        --self-contained-html \
        --cov=src \
        --cov-report=html:{envtmpdir}/coverage_html \
        --cov-report=term-missing \
        --verbose \
        --tb=short \
        tests/performance/

# Change directory to project root for proper import resolution
changedir = {toxinidir}
# Install package in development mode for testing
usedevelop = true
# Pass through environment variables for CI/CD integration
passenv = 
    CI
    TRAVIS
    GITHUB_*
    PYTEST_*
    BENCHMARK_*
    NODE_*

[testenv:py313-flask311-performance]
# Primary Flask 3.1.1 performance testing environment with Python 3.13.3
description = Primary Flask 3.1.1 API performance benchmarking with pytest-benchmark
# Focus on API endpoint performance testing
commands = 
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/api_benchmarks \
        --benchmark-autosave \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/api_benchmark.json \
        --html={envtmpdir}/api_performance_report.html \
        --self-contained-html \
        --verbose \
        tests/performance/test_api_benchmarks.py

[testenv:py313-baseline-comparison]
# Node.js baseline comparison testing environment
description = Flask vs Node.js baseline performance comparison with statistical validation
# Additional environment variables for baseline comparison
setenv = 
    {[testenv]setenv}
    BASELINE_COMPARISON_ENABLED = true
    NODEJS_SERVER_URL = http://localhost:3000
    COMPARISON_TOLERANCE_PERCENT = 10
    STATISTICAL_CONFIDENCE_LEVEL = 0.95
commands = 
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/baseline_comparison \
        --benchmark-autosave \
        --benchmark-compare-fail=min:10% \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/baseline_comparison.json \
        --html={envtmpdir}/baseline_comparison_report.html \
        --self-contained-html \
        --verbose \
        tests/performance/test_baseline_comparison.py

[testenv:py313-concurrent-load]
# Concurrent user load testing environment
description = Concurrent user load testing with thread pool utilization monitoring
# Configure for concurrent testing scenarios
setenv = 
    {[testenv]setenv}
    CONCURRENT_LOAD_TESTING = true
    MAX_CONCURRENT_USERS = 100
    LOAD_TEST_DURATION_SECONDS = 30
    THREAD_POOL_SIZE = 20
commands = 
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/concurrent_load \
        --benchmark-autosave \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/concurrent_load.json \
        --html={envtmpdir}/concurrent_load_report.html \
        --self-contained-html \
        --verbose \
        tests/performance/test_concurrent_load.py

[testenv:py313-memory-profiling]
# Memory profiling and optimization testing environment
description = Python memory profiling with GC pause analysis and optimization validation
# Memory profiling specific configuration
setenv = 
    {[testenv]setenv}
    MEMORY_PROFILING_ENABLED = true
    GC_MONITORING_ENABLED = true
    MEMORY_LEAK_DETECTION = true
    PROFILING_INTERVAL_SECONDS = 0.1
commands = 
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/memory_profiling \
        --benchmark-autosave \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/memory_profiling.json \
        --html={envtmpdir}/memory_profiling_report.html \
        --self-contained-html \
        --verbose \
        tests/performance/test_memory_profiling.py

[testenv:py313-database-benchmarks]
# Database query performance benchmarking environment  
description = SQLAlchemy query performance with sub-100ms validation and connection pool monitoring
# Database-specific performance testing configuration
setenv = 
    {[testenv]setenv}
    DATABASE_BENCHMARKING = true
    SQLALCHEMY_ECHO = true
    CONNECTION_POOL_SIZE = 10
    CONNECTION_POOL_OVERFLOW = 20
    QUERY_TIMEOUT_SECONDS = 5
commands = 
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/database_benchmarks \
        --benchmark-autosave \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/database_benchmarks.json \
        --html={envtmpdir}/database_performance_report.html \
        --self-contained-html \
        --verbose \
        tests/performance/test_database_benchmarks.py

[testenv:py313-authentication-benchmarks] 
# Authentication performance benchmarking environment
description = Flask authentication decorator and Auth0 integration performance validation
# Authentication-specific testing configuration
setenv = 
    {[testenv]setenv}
    AUTHENTICATION_BENCHMARKING = true
    AUTH0_MOCK_ENABLED = true
    SESSION_PERFORMANCE_MONITORING = true
    TOKEN_VALIDATION_TIMEOUT = 5
commands = 
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/auth_benchmarks \
        --benchmark-autosave \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/auth_benchmarks.json \
        --html={envtmpdir}/auth_performance_report.html \
        --self-contained-html \
        --verbose \
        tests/performance/test_authentication_benchmarks.py

[testenv:py313-comprehensive-validation]
# Comprehensive performance validation across all testing scenarios
description = Complete performance validation suite with migration success verification
# Comprehensive testing configuration
setenv = 
    {[testenv]setenv}
    COMPREHENSIVE_VALIDATION = true
    MIGRATION_SUCCESS_VALIDATION = true
    PERFORMANCE_REGRESSION_CHECK = true
    SLA_COMPLIANCE_VALIDATION = true
commands = 
    # Execute complete performance validation suite
    pytest {posargs} \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/comprehensive \
        --benchmark-autosave \
        --benchmark-compare-fail=min:5% \
        --benchmark-compare-fail=mean:10% \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/comprehensive_validation.json \
        --html={envtmpdir}/comprehensive_validation_report.html \
        --self-contained-html \
        --cov=src \
        --cov-report=html:{envtmpdir}/comprehensive_coverage \
        --cov-report=term-missing \
        --verbose \
        --maxfail=5 \
        tests/performance/

[testenv:parallel-performance]
# Parallel execution environment for comprehensive performance testing
description = Parallel performance testing execution with environment isolation
# Parallel execution configuration
deps = 
    {[testenv]deps}
    pytest-xdist[psutil]>=3.5.0
commands = 
    # Execute tests in parallel with optimal worker distribution
    pytest {posargs} \
        --numprocesses=auto \
        --benchmark-storage={env:BENCHMARK_STORAGE_PATH}/parallel \
        --benchmark-autosave \
        --benchmark-json={env:BENCHMARK_STORAGE_PATH}/parallel_execution.json \
        --html={envtmpdir}/parallel_performance_report.html \
        --self-contained-html \
        --verbose \
        --dist=loadscope \
        tests/performance/

# Benchmark configuration for pytest-benchmark integration
[tool:pytest-benchmark]
# Benchmark storage and analysis configuration
storage = {envtmpdir}/benchmark_results
autosave = true
save-data = true
compare-fail = min:5%,mean:10%
warmup = true
warmup-iterations = 3
# Statistical validation configuration
histogram = true
json = true
# Performance comparison settings
compare = true
group-by = group,param,name

# pytest configuration specific to performance testing
[tool:pytest]
# Test discovery and execution configuration
testpaths = tests/performance
python_files = test_*.py
python_classes = Test*
python_functions = test_*
# Marker definitions for performance testing
markers =
    benchmark: mark test as a benchmark
    performance: mark test as performance-related
    baseline: mark test for baseline comparison
    concurrent: mark test for concurrent load testing
    memory: mark test for memory profiling
    database: mark test for database performance
    authentication: mark test for authentication performance
    regression: mark test for performance regression
    sla: mark test for SLA compliance validation
# Benchmark-specific configuration
benchmark-storage = .benchmarks
benchmark-autosave = true
benchmark-json = benchmark.json
# HTML reporting configuration
html-report = performance_report.html
html-self-contained = true
# Coverage configuration for performance testing
cov = src
cov-report = html:coverage_html
cov-report = term-missing
# Logging configuration
log-level = INFO
log-cli = true
log-cli-level = INFO
log-cli-format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log-cli-date-format = %Y-%m-%d %H:%M:%S

# Coverage configuration for performance testing analysis
[coverage:run]
source = src
omit = 
    */tests/*
    */venv/*
    */migrations/*
    */conftest.py
    */test_*.py
branch = true
parallel = true

[coverage:report]
precision = 2
show_missing = true
skip_covered = false
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

[coverage:html]
directory = coverage_html
title = Flask Performance Testing Coverage Report