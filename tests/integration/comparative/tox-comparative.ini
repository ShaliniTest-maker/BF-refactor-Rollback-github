# Tox Comparative Testing Configuration for Node.js vs Flask Migration
# ===================================================================
# 
# This specialized tox configuration orchestrates comprehensive comparative
# testing between Node.js baseline and Flask implementation systems,
# ensuring 100% functional parity during the migration process.
# 
# Key Features:
# - tox 4.26.0 multi-environment comparative testing per Section 4.7.2
# - Parallel Node.js and Flask system execution with real-time comparison
# - Python 3.13.3 and Flask 3.1.1 compatibility validation
# - Virtual environment isolation for reproducible comparative validation
# - pytest-flask 1.3.0 integration for Flask-specific testing capabilities
# - pytest-benchmark 5.1.0 for performance baseline comparison
# - Automated discrepancy detection with real-time comparison reporting
# - CI/CD pipeline integration for automated comparative validation
# - Comprehensive parity validation with 100% equivalence requirement
# 
# Technical Specification References:
# - Section 4.7.2: Comparative Testing Process with automated correction workflow
# - Section 4.7.1: pytest-flask Plugin Configuration and Performance Testing Harness
# - Section 0.2.4: Dependency Migration Table and equivalence requirements
# - Section 8.4: CI/CD Pipeline integration for automated comparative validation
# - Section 3.2: Flask 3.1.1 ecosystem compatibility with Node.js baseline

[tox]
# ===================================================================
# COMPARATIVE TESTING GLOBAL CONFIGURATION
# ===================================================================

# Minimum tox version required per Section 4.7.2 technical specification
minversion = 4.26.0

# Specialized comparative testing environments focusing on Node.js vs Flask validation
# Comprehensive coverage of API, performance, workflow, and integration parity
envlist = 
    nodejs-baseline-capture
    flask-target-validation
    comparative-api-parity
    comparative-performance
    comparative-workflows
    comparative-database
    comparative-auth
    parallel-systems-validation
    discrepancy-analysis
    parity-validation-report
    ci-comparative-pipeline

# Skip missing interpreters to allow selective comparative environment testing
skip_missing_interpreters = true

# Parallel execution for improved comparative testing performance
# Enables concurrent Node.js and Flask system provisioning per Section 4.7.2
parallel_show_output = true

# Isolated build to ensure reproducible comparative test environments
isolated_build = true

# Default comparative test working directory
testpaths = tests/integration/comparative

# ===================================================================
# DEFAULT COMPARATIVE TEST ENVIRONMENT CONFIGURATION
# ===================================================================

[testenv]
# Base Python interpreter version per technical specification
basepython = python3.13

# Working directory for all comparative test environments
changedir = {toxinidir}

# Core dependencies for comparative testing
deps = 
    -r{toxinidir}/tests/integration/requirements-test.txt
    # Comparative testing specific dependencies
    deepdiff>=8.0.0
    jsoncompare>=1.0.0
    requests>=2.32.0
    responses>=0.25.0
    aiohttp>=3.10.0
    httpx>=0.28.0

# Environment variables for comparative testing configuration
setenv =
    # Flask application configuration for comparative testing
    FLASK_ENV = testing
    FLASK_APP = src.app:create_app
    FLASK_DEBUG = 0
    FLASK_TESTING = true
    
    # Comparative testing mode activation
    COMPARATIVE_TEST_MODE = true
    PARALLEL_SYSTEM_TESTING = true
    
    # System endpoint configuration for comparative testing
    NODEJS_BASELINE_URL = http://localhost:3000
    FLASK_TARGET_URL = http://localhost:5000
    NODEJS_BASELINE_PORT = 3000
    FLASK_TARGET_PORT = 5000
    
    # Database configuration for isolated comparative testing
    DATABASE_URL = sqlite:///:memory:
    TEST_DATABASE_URL = sqlite:///:memory:
    NODEJS_DB_URL = mongodb://localhost:27017/test_nodejs_baseline
    FLASK_DB_URL = sqlite:///test_flask_comparative.db
    
    # Authentication testing configuration for both systems
    SECRET_KEY = test-secret-key-for-comparative-testing
    AUTH0_DOMAIN = test-auth0-domain.auth0.com
    AUTH0_CLIENT_ID = test-client-id
    AUTH0_CLIENT_SECRET = test-client-secret
    
    # Comparative testing validation thresholds
    PARITY_THRESHOLD = 0.0
    RESPONSE_TOLERANCE = 0.001
    PERFORMANCE_TOLERANCE = 0.1
    API_RESPONSE_TIMEOUT = 30
    
    # Discrepancy detection configuration
    ENABLE_DISCREPANCY_DETECTION = true
    AUTOMATED_CORRECTION_ENABLED = true
    REAL_TIME_COMPARISON = true
    
    # Performance comparison configuration
    BENCHMARK_BASELINE_MODE = nodejs
    BENCHMARK_TARGET_MODE = flask
    BENCHMARK_COMPARISON_ENABLED = true
    
    # Testing framework configuration
    PYTEST_CURRENT_TEST = {envname}
    TOX_ENV_NAME = {envname}
    COMPARATIVE_REPORT_FORMAT = json,html,xml

# Default comparative test command
commands = 
    pytest {posargs:tests/integration/comparative} \
        --verbose \
        --tb=short \
        --strict-markers \
        --strict-config \
        --junit-xml={toxinidir}/test-results/comparative/{envname}/junit.xml

# Recreate virtual environments for clean comparative testing
recreate = false

# ===================================================================
# NODE.JS BASELINE CAPTURE ENVIRONMENT
# ===================================================================

[testenv:nodejs-baseline-capture]
# Node.js baseline response capture and reference data establishment
# Creates ground truth data for comparative validation per Section 4.7.2

description = Node.js baseline response capture for comparative validation

deps = 
    {[testenv]deps}
    # Additional baseline capture dependencies
    pyyaml>=6.0.2
    jsonschema>=4.23.0

setenv = 
    {[testenv]setenv}
    # Baseline capture specific configuration
    BASELINE_CAPTURE_MODE = true
    NODEJS_SYSTEM_ACTIVE = true
    BASELINE_STORAGE_PATH = {toxinidir}/test-results/baseline-data
    BASELINE_REFRESH_ENABLED = true
    
    # Comprehensive data capture configuration
    CAPTURE_API_RESPONSES = true
    CAPTURE_PERFORMANCE_METRICS = true
    CAPTURE_WORKFLOW_STATES = true
    CAPTURE_DATABASE_OPERATIONS = true
    CAPTURE_AUTH_FLOWS = true

commands = 
    # Ensure baseline data directory exists
    python -c "import os; os.makedirs('{toxinidir}/test-results/baseline-data', exist_ok=True)"
    
    # Node.js system baseline capture
    pytest tests/integration/comparative/baseline_capture.py \
        --verbose \
        --tb=long \
        --durations=20 \
        --junit-xml={toxinidir}/test-results/comparative/nodejs-baseline/junit.xml \
        --html={toxinidir}/test-results/comparative/nodejs-baseline/report.html \
        --self-contained-html \
        -m "baseline_capture" \
        {posargs}
    
    # Baseline data validation and schema verification
    pytest tests/integration/comparative/test_baseline_validation.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/baseline-validation/junit.xml \
        {posargs}

# ===================================================================
# FLASK TARGET VALIDATION ENVIRONMENT
# ===================================================================

[testenv:flask-target-validation]
# Flask implementation validation against captured baseline data
# Ensures Flask system readiness for comparative testing per Section 4.7.2

description = Flask target system validation and readiness verification

deps = 
    {[testenv]deps}

setenv = 
    {[testenv]setenv}
    # Flask target validation configuration
    FLASK_VALIDATION_MODE = true
    FLASK_SYSTEM_ACTIVE = true
    BASELINE_COMPARISON_ENABLED = true
    
    # Flask-specific testing configuration
    FLASK_BLUEPRINT_TESTING = true
    FLASK_SERVICE_LAYER_TESTING = true
    FLASK_SQLALCHEMY_TESTING = true

commands = 
    # Flask application initialization validation
    pytest tests/integration/comparative/test_flask_readiness.py \
        --verbose \
        --tb=long \
        --durations=20 \
        --junit-xml={toxinidir}/test-results/comparative/flask-validation/junit.xml \
        --html={toxinidir}/test-results/comparative/flask-validation/report.html \
        --self-contained-html \
        -m "flask_validation" \
        {posargs}
    
    # Flask system health check validation
    pytest tests/integration/comparative/test_flask_health.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/flask-health/junit.xml \
        {posargs}

# ===================================================================
# COMPARATIVE API PARITY TESTING
# ===================================================================

[testenv:comparative-api-parity]
# Comprehensive API endpoint comparison between Node.js and Flask
# Validates 100% API response format compatibility per Section 4.7.2

description = API endpoint parity validation between Node.js and Flask systems

deps = 
    {[testenv]deps}
    # API testing specific dependencies
    jsonschema>=4.23.0
    deepdiff>=8.0.0
    requests-mock>=1.12.0

setenv = 
    {[testenv]setenv}
    # API parity testing configuration
    API_PARITY_TESTING = true
    PARALLEL_API_EXECUTION = true
    API_CONTRACT_VALIDATION = true
    
    # Response comparison configuration
    RESPONSE_FORMAT_VALIDATION = true
    HTTP_STATUS_CODE_VALIDATION = true
    RESPONSE_HEADER_VALIDATION = true
    RESPONSE_TIMING_COMPARISON = true
    
    # Error handling validation
    ERROR_RESPONSE_VALIDATION = true
    EXCEPTION_HANDLING_COMPARISON = true

commands = 
    # Comprehensive API response comparison testing
    pytest tests/integration/comparative/test_comparative_api.py \
        --verbose \
        --tb=long \
        --durations=30 \
        --junit-xml={toxinidir}/test-results/comparative/api-parity/junit.xml \
        --html={toxinidir}/test-results/comparative/api-parity/report.html \
        --self-contained-html \
        -m "api_parity" \
        {posargs}
    
    # API contract compliance validation
    pytest tests/integration/comparative/test_api_contracts.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/api-contracts/junit.xml \
        -m "contract_validation" \
        {posargs}
    
    # Error handling consistency validation
    pytest tests/integration/comparative/test_error_handling_parity.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/error-handling/junit.xml \
        -m "error_handling" \
        {posargs}

# ===================================================================
# COMPARATIVE PERFORMANCE TESTING
# ===================================================================

[testenv:comparative-performance]
# Performance benchmarking comparison between Node.js and Flask
# Validates equivalent or improved performance metrics per Section 4.7.1

description = Performance benchmarking comparison with pytest-benchmark 5.1.0

deps = 
    {[testenv]deps}
    # Performance testing dependencies
    memory-profiler>=0.61.0
    psutil>=6.1.0
    py-spy>=0.3.14

setenv = 
    {[testenv]setenv}
    # Performance comparison configuration
    PERFORMANCE_COMPARISON_MODE = true
    BENCHMARK_BASELINE_SYSTEM = nodejs
    BENCHMARK_TARGET_SYSTEM = flask
    
    # Benchmark configuration per Section 4.7.1
    BENCHMARK_ONLY = true
    BENCHMARK_SKIP = false
    BENCHMARK_MIN_TIME = 0.000005
    BENCHMARK_MIN_ROUNDS = 5
    BENCHMARK_MAX_TIME = 1.0
    
    # Memory profiling configuration
    MEMORY_PROFILER_BACKEND = psutil
    MEMORY_USAGE_COMPARISON = true
    
    # Concurrent load testing configuration
    CONCURRENT_USER_TESTING = true
    LOAD_TEST_USER_COUNT = 100
    LOAD_TEST_DURATION = 60

commands = 
    # API endpoint performance benchmarking comparison
    pytest tests/integration/comparative/test_comparative_performance.py \
        --benchmark-only \
        --benchmark-verbose \
        --benchmark-sort=mean \
        --benchmark-min-time=0.000005 \
        --benchmark-min-rounds=5 \
        --benchmark-max-time=1.0 \
        --benchmark-save=comparative_performance_baseline \
        --benchmark-save-data \
        --benchmark-histogram={toxinidir}/test-results/comparative/performance/histograms/ \
        --junit-xml={toxinidir}/test-results/comparative/performance/junit.xml \
        --html={toxinidir}/test-results/comparative/performance/report.html \
        --self-contained-html \
        -m "performance_comparison" \
        {posargs}
    
    # Memory usage profiling comparison
    pytest tests/integration/comparative/test_memory_comparison.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/memory/junit.xml \
        -m "memory_profiling" \
        {posargs}
    
    # Concurrent user load testing comparison
    pytest tests/integration/comparative/test_load_comparison.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/load-testing/junit.xml \
        -m "load_testing" \
        {posargs}

# ===================================================================
# COMPARATIVE WORKFLOW TESTING
# ===================================================================

[testenv:comparative-workflows]
# Business logic workflow comparison ensuring identical execution patterns
# Validates Service Layer pattern implementation per Section 4.7.2

description = Business logic workflow comparison between systems

deps = 
    {[testenv]deps}
    # Workflow testing dependencies
    hypothesis>=6.115.0
    factory-boy>=3.3.1

setenv = 
    {[testenv]setenv}
    # Workflow comparison configuration
    WORKFLOW_COMPARISON_MODE = true
    SERVICE_LAYER_VALIDATION = true
    BUSINESS_LOGIC_COMPARISON = true
    
    # Transaction handling validation
    TRANSACTION_COMPARISON = true
    STATE_MANAGEMENT_VALIDATION = true
    WORKFLOW_ORCHESTRATION_TESTING = true
    
    # Business rule compliance testing
    BUSINESS_RULE_VALIDATION = true
    WORKFLOW_STATE_TRACKING = true

commands = 
    # Comprehensive business logic workflow comparison
    pytest tests/integration/comparative/test_comparative_workflows.py \
        --verbose \
        --tb=long \
        --durations=30 \
        --junit-xml={toxinidir}/test-results/comparative/workflows/junit.xml \
        --html={toxinidir}/test-results/comparative/workflows/report.html \
        --self-contained-html \
        -m "workflow_comparison" \
        {posargs}
    
    # Service Layer pattern validation
    pytest tests/integration/comparative/test_service_layer_parity.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/service-layer/junit.xml \
        -m "service_layer" \
        {posargs}
    
    # Transaction handling consistency validation
    pytest tests/integration/comparative/test_transaction_parity.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/transactions/junit.xml \
        -m "transaction_handling" \
        {posargs}

# ===================================================================
# COMPARATIVE DATABASE TESTING
# ===================================================================

[testenv:comparative-database]
# Database operation comparison between Node.js MongoDB and Flask SQLAlchemy
# Validates data consistency and query result equivalence

description = Database operation comparison and data consistency validation

deps = 
    {[testenv]deps}
    # Database testing dependencies
    pymongo>=4.10.1
    SQLAlchemy>=2.0.0,<3.0.0
    Alembic>=1.13.0

setenv = 
    {[testenv]setenv}
    # Database comparison configuration
    DATABASE_COMPARISON_MODE = true
    MONGODB_COMPARISON_ENABLED = true
    SQLALCHEMY_COMPARISON_ENABLED = true
    
    # Query result validation
    QUERY_RESULT_COMPARISON = true
    DATA_INTEGRITY_VALIDATION = true
    RELATIONSHIP_MAPPING_VALIDATION = true
    
    # Migration validation
    DATABASE_MIGRATION_VALIDATION = true
    SCHEMA_CONSISTENCY_CHECKING = true

commands = 
    # Database query result comparison
    pytest tests/integration/comparative/test_database_parity.py \
        --verbose \
        --tb=long \
        --durations=30 \
        --junit-xml={toxinidir}/test-results/comparative/database/junit.xml \
        --html={toxinidir}/test-results/comparative/database/report.html \
        --self-contained-html \
        -m "database_comparison" \
        {posargs}
    
    # Data model relationship validation
    pytest tests/integration/comparative/test_model_relationships.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/models/junit.xml \
        -m "model_validation" \
        {posargs}

# ===================================================================
# COMPARATIVE AUTHENTICATION TESTING
# ===================================================================

[testenv:comparative-auth]
# Authentication and authorization comparison between systems
# Validates security posture preservation per Section 4.7.2

description = Authentication and authorization comparison validation

deps = 
    {[testenv]deps}
    # Authentication testing dependencies
    PyJWT>=2.10.0
    cryptography>=44.0.0

setenv = 
    {[testenv]setenv}
    # Authentication comparison configuration
    AUTH_COMPARISON_MODE = true
    SESSION_MANAGEMENT_VALIDATION = true
    SECURITY_POSTURE_VALIDATION = true
    
    # Auth0 integration testing
    AUTH0_INTEGRATION_TESTING = true
    JWT_TOKEN_VALIDATION = true
    SESSION_COOKIE_VALIDATION = true
    
    # Security validation
    ITSDANGEROUS_VALIDATION = true
    FLASK_LOGIN_VALIDATION = true

commands = 
    # Authentication flow comparison
    pytest tests/integration/comparative/test_auth_parity.py \
        --verbose \
        --tb=long \
        --durations=30 \
        --junit-xml={toxinidir}/test-results/comparative/auth/junit.xml \
        --html={toxinidir}/test-results/comparative/auth/report.html \
        --self-contained-html \
        -m "auth_comparison" \
        {posargs}
    
    # Session management validation
    pytest tests/integration/comparative/test_session_parity.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/sessions/junit.xml \
        -m "session_validation" \
        {posargs}

# ===================================================================
# PARALLEL SYSTEMS VALIDATION
# ===================================================================

[testenv:parallel-systems-validation]
# Coordinated parallel execution of Node.js and Flask systems
# Real-time comparative validation per Section 4.7.2 requirements

description = Parallel Node.js and Flask system validation with real-time comparison

deps = 
    {[testenv]deps}
    # Parallel execution dependencies
    pytest-xdist>=3.6.0
    pytest-asyncio>=0.24.0

setenv = 
    {[testenv]setenv}
    # Parallel systems configuration
    PARALLEL_SYSTEMS_MODE = true
    REAL_TIME_COMPARISON = true
    COORDINATED_EXECUTION = true
    
    # System coordination configuration
    NODEJS_FLASK_COORDINATION = true
    SYNCHRONIZED_TESTING = true
    PARALLEL_EXECUTION_TIMEOUT = 300
    
    # Real-time monitoring
    REAL_TIME_MONITORING = true
    LIVE_COMPARISON_REPORTING = true

commands = 
    # Coordinated parallel system testing
    pytest tests/integration/comparative/test_parallel_systems.py \
        -n auto \
        --dist=loadscope \
        --verbose \
        --tb=long \
        --durations=30 \
        --junit-xml={toxinidir}/test-results/comparative/parallel-systems/junit.xml \
        --html={toxinidir}/test-results/comparative/parallel-systems/report.html \
        --self-contained-html \
        -m "parallel_systems" \
        {posargs}
    
    # Real-time comparison validation
    pytest tests/integration/comparative/test_realtime_comparison.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/realtime/junit.xml \
        -m "realtime_comparison" \
        {posargs}

# ===================================================================
# DISCREPANCY ANALYSIS ENVIRONMENT
# ===================================================================

[testenv:discrepancy-analysis]
# Automated discrepancy detection and analysis workflow
# Triggers correction procedures per Section 4.7.2 automated correction workflow

description = Automated discrepancy detection and analysis with correction workflow

deps = 
    {[testenv]deps}
    # Analysis and reporting dependencies
    pandas>=2.2.0
    numpy>=2.1.0
    matplotlib>=3.9.0
    seaborn>=0.13.0

setenv = 
    {[testenv]setenv}
    # Discrepancy analysis configuration
    DISCREPANCY_ANALYSIS_MODE = true
    AUTOMATED_CORRECTION_ENABLED = true
    ROOT_CAUSE_ANALYSIS = true
    
    # Analysis configuration
    DETAILED_DIFF_ANALYSIS = true
    PERFORMANCE_DEVIATION_ANALYSIS = true
    DATABASE_VARIANCE_ANALYSIS = true
    ERROR_PATTERN_ANALYSIS = true
    
    # Correction workflow configuration
    CORRECTION_WORKFLOW_ENABLED = true
    AUTOMATED_FIX_GENERATION = true
    FLASK_ADJUSTMENT_AUTOMATION = true

commands = 
    # Comprehensive discrepancy analysis
    pytest tests/integration/comparative/results_analyzer.py \
        --verbose \
        --tb=long \
        --durations=30 \
        --junit-xml={toxinidir}/test-results/comparative/discrepancy-analysis/junit.xml \
        --html={toxinidir}/test-results/comparative/discrepancy-analysis/report.html \
        --self-contained-html \
        -m "discrepancy_analysis" \
        {posargs}
    
    # Automated correction workflow execution
    pytest tests/integration/comparative/test_correction_workflow.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/correction/junit.xml \
        -m "correction_workflow" \
        {posargs}

# ===================================================================
# PARITY VALIDATION REPORT GENERATION
# ===================================================================

[testenv:parity-validation-report]
# Comprehensive parity validation report generation
# Consolidates all comparative testing results per Section 4.7.2

description = Comprehensive parity validation report generation and consolidation

deps = 
    {[testenv]deps}
    # Reporting dependencies
    jinja2>=3.1.2
    reportlab>=4.2.5
    weasyprint>=62.3

setenv = 
    {[testenv]setenv}
    # Report generation configuration
    REPORT_GENERATION_MODE = true
    COMPREHENSIVE_REPORTING = true
    PARITY_STATUS_REPORTING = true
    
    # Report format configuration
    GENERATE_HTML_REPORT = true
    GENERATE_PDF_REPORT = true
    GENERATE_JSON_REPORT = true
    GENERATE_XML_REPORT = true
    
    # Report content configuration
    INCLUDE_PERFORMANCE_METRICS = true
    INCLUDE_API_COVERAGE = true
    INCLUDE_DISCREPANCY_ANALYSIS = true
    INCLUDE_MIGRATION_STATUS = true

commands = 
    # Comprehensive parity validation report generation
    pytest tests/integration/comparative/test_report_generation.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/reporting/junit.xml \
        -m "report_generation" \
        {posargs}
    
    # Migration status consolidation
    python tests/integration/comparative/generate_migration_report.py \
        --input-dir {toxinidir}/test-results/comparative \
        --output-dir {toxinidir}/test-results/migration-report \
        --format all \
        {posargs}

# ===================================================================
# CI/CD COMPARATIVE PIPELINE INTEGRATION
# ===================================================================

[testenv:ci-comparative-pipeline]
# Specialized CI/CD environment for automated comparative testing pipeline
# Optimized for GitHub Actions automation per Section 8.4 CI/CD Pipeline

description = CI/CD comparative testing pipeline integration

deps = 
    {[testenv]deps}

setenv = 
    {[testenv]setenv}
    # CI/CD specific configuration
    CI = true
    GITHUB_ACTIONS = true
    COMPARATIVE_CI_MODE = true
    
    # Optimized CI settings
    PYTEST_DISABLE_PLUGIN_AUTOLOAD = true
    PYTEST_TIMEOUT = 600
    COMPARATIVE_TEST_TIMEOUT = 900
    
    # CI pipeline integration
    AUTOMATED_PIPELINE_EXECUTION = true
    PIPELINE_FAILURE_THRESHOLD = 5
    AUTOMATED_ROLLBACK_ENABLED = true

commands = 
    # Streamlined comparative testing for CI/CD
    pytest tests/integration/comparative/ \
        --verbose \
        --tb=short \
        --maxfail=5 \
        --timeout=600 \
        --junit-xml={toxinidir}/test-results/comparative/ci-pipeline/junit.xml \
        --cov=tests/integration/comparative \
        --cov-report=xml:{toxinidir}/test-results/comparative/ci-pipeline/coverage.xml \
        -m "not slow and not manual" \
        {posargs}
    
    # CI-specific parity validation
    pytest tests/integration/comparative/test_ci_parity_validation.py \
        --verbose \
        --tb=short \
        --junit-xml={toxinidir}/test-results/comparative/ci-validation/junit.xml \
        -m "ci_validation" \
        {posargs}

# ===================================================================
# COMPARATIVE TESTING UTILITIES
# ===================================================================

[testenv:comparative-clean]
# Comparative testing environment cleanup utility
# Removes comparative test artifacts and baseline data

description = Clean comparative testing artifacts and baseline data

deps = 

commands = 
    python -c "
import shutil
import os
import glob

# Clean comparative test results
dirs_to_clean = [
    'test-results/comparative',
    'test-results/baseline-data',
    'test-results/migration-report'
]

for dir_path in dirs_to_clean:
    if os.path.exists(dir_path):
        shutil.rmtree(dir_path)
        print(f'Cleaned: {dir_path}')

# Clean comparative cache files
cache_patterns = [
    '.pytest_cache/comparative',
    '.coverage.comparative*',
    '*.comparative.log'
]

for pattern in cache_patterns:
    for f in glob.glob(pattern):
        if os.path.isfile(f):
            os.remove(f)
        elif os.path.isdir(f):
            shutil.rmtree(f)

print('Comparative testing artifacts cleaned successfully')
"

# ===================================================================
# DEVELOPMENT AND DEBUGGING ENVIRONMENTS
# ===================================================================

[testenv:comparative-dev]
# Development environment for interactive comparative testing
# Provides debugging and development workflow integration

description = Development environment for interactive comparative testing

deps = 
    {[testenv]deps}
    ipython>=8.0.0
    pdbpp>=0.10.3
    ipdb>=0.13.13

setenv = 
    {[testenv]setenv}
    # Development specific configuration
    FLASK_DEBUG = 1
    DEVELOPMENT_MODE = true
    COMPARATIVE_DEBUG_MODE = true
    
    # Debug logging configuration
    COMPARATIVE_LOG_LEVEL = DEBUG
    DETAILED_LOGGING = true

commands = 
    # Interactive comparative testing environment
    python -c "print('Comparative testing development environment ready')"
    python -c "print('Available commands:')"
    python -c "print('  - pytest tests/integration/comparative/test_comparative_api.py -v -s')"
    python -c "print('  - python tests/integration/comparative/baseline_capture.py')"
    python -c "print('  - python tests/integration/comparative/results_analyzer.py')"
    
    # Available for custom commands
    {posargs:python -c "print('Use: tox -e comparative-dev -- <your-command>')"}

# ===================================================================
# CONFIGURATION VALIDATION AND TESTING
# ===================================================================

[testenv:comparative-validate]
# Comparative testing configuration validation
# Validates tox configuration and environment setup for comparative testing

description = Validate comparative testing configuration and environment setup

deps = 
    tox>=4.26.0
    pytest>=8.0.0

commands = 
    # Validate comparative tox configuration
    tox --help-ini
    
    # List comparative environments
    tox --listenvs
    
    # Validate comparative testing setup
    python -c "
import sys
import os
import pkg_resources

print('=== Comparative Testing Environment Validation ===')
print(f'Python version: {sys.version}')
print(f'Platform: {sys.platform}')
print(f'Working directory: {os.getcwd()}')

# Validate key dependencies for comparative testing
dependencies = [
    'Flask', 'pytest', 'pytest-flask', 'pytest-benchmark',
    'deepdiff', 'jsoncompare', 'requests'
]

for dep in dependencies:
    try:
        version = pkg_resources.get_distribution(dep).version
        print(f'{dep} version: {version}')
    except:
        print(f'{dep}: NOT INSTALLED')

# Validate environment variables
env_vars = [
    'COMPARATIVE_TEST_MODE', 'NODEJS_BASELINE_URL', 'FLASK_TARGET_URL'
]

print('\n=== Environment Variables ===')
for var in env_vars:
    value = os.environ.get(var, 'NOT SET')
    print(f'{var}: {value}')

# Validate test directory structure
test_dirs = [
    'tests/integration/comparative',
    'test-results'
]

print('\n=== Directory Structure ===')
for dir_path in test_dirs:
    exists = 'EXISTS' if os.path.exists(dir_path) else 'MISSING'
    print(f'{dir_path}: {exists}')

print('\nComparative testing configuration validation complete')
"

# ===================================================================
# QUICK COMPARATIVE TESTING ENVIRONMENTS
# ===================================================================

[testenv:quick-comparative]
# Quick comparative testing for rapid development feedback
# Minimal comparative test suite for fast validation

description = Quick comparative testing for development feedback

deps = 
    {[testenv]deps}

setenv = 
    {[testenv]setenv}
    # Quick testing configuration
    QUICK_COMPARATIVE_MODE = true

commands = 
    # Quick comparative smoke tests
    pytest tests/integration/comparative/test_comparative_api.py::TestComparativeAPI::test_basic_api_parity \
        --verbose \
        --tb=short \
        -m "smoke_test" \
        {posargs}

# ===================================================================
# COMPREHENSIVE COMPARATIVE TESTING SUITE
# ===================================================================

[testenv:full-comparative]
# Complete comprehensive comparative testing suite
# Executes all comparative testing environments in sequence

description = Complete comprehensive comparative testing execution

deps = 
    {[testenv]deps}

setenv = 
    {[testenv]setenv}
    # Full comparative suite configuration
    FULL_COMPARATIVE_SUITE = true
    COMPREHENSIVE_VALIDATION = true

commands = 
    # Execute full comparative testing pipeline
    python -c "print('=== Starting Comprehensive Comparative Testing Suite ===')"
    
    # Execute baseline capture
    pytest tests/integration/comparative/baseline_capture.py -v
    
    # Execute Flask validation
    pytest tests/integration/comparative/test_flask_readiness.py -v
    
    # Execute API parity testing
    pytest tests/integration/comparative/test_comparative_api.py -v
    
    # Execute performance comparison
    pytest tests/integration/comparative/test_comparative_performance.py -v --benchmark-only
    
    # Execute workflow comparison
    pytest tests/integration/comparative/test_comparative_workflows.py -v
    
    # Execute discrepancy analysis
    pytest tests/integration/comparative/results_analyzer.py -v
    
    # Generate comprehensive report
    python tests/integration/comparative/generate_migration_report.py
    
    python -c "print('=== Comprehensive Comparative Testing Suite Complete ===')"

# ===================================================================
# SECTION FOOTER AND USAGE DOCUMENTATION
# ===================================================================

# Comparative Testing Usage Examples:
# ==================================
# 
# Capture Node.js baseline data:
#   tox -e nodejs-baseline-capture
# 
# Validate Flask target system:
#   tox -e flask-target-validation
# 
# Run API parity comparison:
#   tox -e comparative-api-parity
# 
# Run performance benchmarking:
#   tox -e comparative-performance
# 
# Run workflow comparison:
#   tox -e comparative-workflows
# 
# Run parallel systems validation:
#   tox -e parallel-systems-validation
# 
# Analyze discrepancies:
#   tox -e discrepancy-analysis
# 
# Generate parity report:
#   tox -e parity-validation-report
# 
# Run CI/CD comparative pipeline:
#   tox -e ci-comparative-pipeline
# 
# Clean comparative artifacts:
#   tox -e comparative-clean
# 
# Development environment:
#   tox -e comparative-dev
# 
# Quick comparative testing:
#   tox -e quick-comparative
# 
# Full comprehensive suite:
#   tox -e full-comparative
# 
# Validate configuration:
#   tox -e comparative-validate
# 
# Run specific comparative test:
#   tox -e comparative-api-parity -- tests/integration/comparative/test_specific.py -v -s
# 
# Run with custom parameters:
#   tox -e comparative-performance -- --benchmark-min-rounds=10 --benchmark-max-time=2.0
#
# This specialized comparative testing configuration enables comprehensive
# validation of Flask 3.1.1 migration against Node.js baseline with
# automated discrepancy detection, real-time comparison reporting, and
# integrated correction workflows per Section 4.7.2 technical specification.