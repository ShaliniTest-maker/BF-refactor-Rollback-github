# Performance Testing and Benchmarking Workflow
# Validates Flask implementation performance against Node.js baseline metrics
# using pytest-benchmark 5.1.0 for comprehensive performance analysis
#
# This workflow ensures performance parity during migration by measuring:
# - API response times with sub-200ms requirement validation per Section 4.11.1
# - Memory usage optimization compared to Node.js baseline per Section 4.7.1
# - Database query performance with sub-100ms SQLAlchemy benchmarks per Section 4.11.1
# - Concurrent user load handling with statistical validation per Section 8.4.3
# - Authentication performance with sub-150ms response times per Section 4.11.1
#
# Performance regression detection with automated alerting ensures continuous
# validation of migration success criteria and system performance optimization

name: Performance Testing and Benchmarking

on:
  # Trigger on code changes to ensure performance regression detection
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/performance/**'
      - 'requirements.txt'
      - '.github/workflows/performance-testing.yml'
  
  # Pull request validation for performance impact assessment
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/performance/**'
      - 'requirements.txt'
  
  # Manual trigger for comprehensive performance validation
  workflow_dispatch:
    inputs:
      performance_mode:
        description: 'Performance testing mode'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - 'comprehensive'
          - 'baseline-comparison'
          - 'regression-only'
      
      baseline_comparison:
        description: 'Enable Node.js baseline comparison'
        required: false
        default: true
        type: boolean
      
      memory_profiling:
        description: 'Enable detailed memory profiling'
        required: false
        default: true
        type: boolean

  # Scheduled performance monitoring for trend analysis
  schedule:
    # Run comprehensive performance tests twice daily at 06:00 and 18:00 UTC
    - cron: '0 6,18 * * *'

# Environment variables for performance testing configuration
env:
  PYTHON_VERSION: '3.13.3'
  FLASK_ENV: 'testing'
  FLASK_APP: 'src.app:create_app'
  
  # Performance test configuration per Section 4.11.1
  PERFORMANCE_API_TIMEOUT: '200'        # Sub-200ms API response requirement
  PERFORMANCE_DB_TIMEOUT: '100'         # Sub-100ms database query requirement  
  PERFORMANCE_AUTH_TIMEOUT: '150'       # Sub-150ms authentication requirement
  
  # pytest-benchmark configuration per Section 4.7.1
  BENCHMARK_DISABLE_GC: 'false'         # Enable GC for realistic performance testing
  BENCHMARK_WARMUP_ITERATIONS: '5'      # Warmup iterations for statistical accuracy
  BENCHMARK_MIN_ROUNDS: '10'            # Minimum benchmark rounds for confidence
  BENCHMARK_MAX_TIME: '60'              # Maximum benchmark time per test
  
  # Memory profiling configuration per Section 6.5.1.1
  MEMORY_PROFILING_ENABLED: 'true'      # Enable comprehensive memory analysis
  MEMORY_LEAK_DETECTION: 'true'         # Enable memory leak detection
  
  # Database configuration for performance testing
  DATABASE_URL: 'postgresql://test_user:test_pass@localhost:5432/test_performance_db'
  DATABASE_POOL_SIZE: '20'              # Connection pool size for load testing
  
  # Baseline comparison configuration per Section 4.7.2
  BASELINE_STORAGE_PATH: 'performance-baselines'
  NODEJS_BASELINE_URL: 'http://localhost:3000'  # Node.js system for comparison

# Performance testing job with comprehensive validation
jobs:
  performance-testing:
    name: Performance Testing and Benchmarking
    runs-on: ubuntu-24.04
    timeout-minutes: 60

    # Service containers for performance testing infrastructure
    services:
      # PostgreSQL database for SQLAlchemy performance testing
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: test_performance_db
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_pass
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      # Redis for session management performance testing
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3
        ports:
          - 6379:6379

    steps:
      # Source code checkout for performance testing
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for performance trend analysis

      # Python 3.13.3 environment setup per Section 3.1.1
      - name: Setup Python 3.13.3 Environment
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            tests/performance/requirements-performance.txt

      # Install system dependencies for performance monitoring
      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            libpq-dev \
            postgresql-client \
            htop \
            iotop \
            nethogs \
            psmisc \
            procps
          
          # Install memory profiling tools
          sudo apt-get install -y \
            valgrind \
            massif-visualizer

      # Install Python dependencies with performance testing packages
      - name: Install Python Dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          
          # Install core application dependencies
          pip install -r requirements.txt
          
          # Install performance testing dependencies per Section 4.7.1
          pip install pytest-benchmark==5.1.0
          pip install tox==4.26.0
          
          # Install memory profiling dependencies per Section 6.5.1.1
          pip install memory-profiler==0.60.0
          pip install pympler==0.9
          pip install psutil==5.9.6
          
          # Install statistical analysis packages
          pip install scipy==1.11.4
          pip install matplotlib==3.8.2
          pip install seaborn==0.13.0
          
          # Install load testing dependencies
          pip install locust==2.17.0
          pip install aiohttp==3.9.1
          
          # Install performance test specific requirements
          if [ -f tests/performance/requirements-performance.txt ]; then
            pip install -r tests/performance/requirements-performance.txt
          fi

      # Verify Flask application factory setup
      - name: Verify Flask Application Setup
        run: |
          export FLASK_APP=${{ env.FLASK_APP }}
          export FLASK_ENV=${{ env.FLASK_ENV }}
          
          # Test Flask application factory initialization
          python -c "
          from src.app import create_app
          app = create_app()
          print(f'Flask app created successfully: {app}')
          print(f'Flask version: {app.__class__.__module__}')
          "
          
          # Verify Flask CLI commands
          flask --version
          flask routes --sort rule

      # Database initialization for performance testing
      - name: Initialize Performance Test Database
        run: |
          export FLASK_APP=${{ env.FLASK_APP }}
          export DATABASE_URL=${{ env.DATABASE_URL }}
          
          # Wait for PostgreSQL to be fully ready
          until pg_isready -h localhost -p 5432 -U test_user; do
            echo "Waiting for PostgreSQL..."
            sleep 2
          done
          
          # Initialize database schema for performance testing
          flask db upgrade || echo "No migrations to run"
          
          # Verify database connection
          python -c "
          from src.app import create_app
          from src.models import db
          app = create_app()
          with app.app_context():
              try:
                  db.create_all()
                  print('Database initialized successfully')
              except Exception as e:
                  print(f'Database initialization warning: {e}')
          "

      # System performance baseline collection
      - name: Collect System Performance Baseline
        run: |
          echo "=== System Performance Baseline ===" > system-baseline.log
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> system-baseline.log
          echo "" >> system-baseline.log
          
          echo "=== CPU Information ===" >> system-baseline.log
          lscpu >> system-baseline.log
          echo "" >> system-baseline.log
          
          echo "=== Memory Information ===" >> system-baseline.log
          free -h >> system-baseline.log
          echo "" >> system-baseline.log
          
          echo "=== Disk Information ===" >> system-baseline.log
          df -h >> system-baseline.log
          echo "" >> system-baseline.log
          
          echo "=== Process Information ===" >> system-baseline.log
          ps aux --sort=-%cpu | head -10 >> system-baseline.log

      # API Response Time Benchmarking per Section 4.7.1
      - name: API Response Time Benchmarking
        run: |
          echo "Starting API response time benchmarking with pytest-benchmark 5.1.0..."
          
          # Export performance test configuration
          export FLASK_APP=${{ env.FLASK_APP }}
          export FLASK_ENV=${{ env.FLASK_ENV }}
          export DATABASE_URL=${{ env.DATABASE_URL }}
          
          # Run API performance benchmarks with sub-200ms validation
          python -m pytest tests/performance/test_api_benchmarks.py \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=${{ env.BENCHMARK_WARMUP_ITERATIONS }} \
            --benchmark-min-rounds=${{ env.BENCHMARK_MIN_ROUNDS }} \
            --benchmark-max-time=${{ env.BENCHMARK_MAX_TIME }} \
            --benchmark-json=api-benchmarks.json \
            --benchmark-histogram=api-response-histogram \
            --benchmark-save=api-benchmarks \
            --benchmark-save-data \
            -v --tb=short
          
          # Validate API response time requirements per Section 4.11.1
          python -c "
          import json
          import sys
          
          # Load benchmark results
          with open('api-benchmarks.json', 'r') as f:
              results = json.load(f)
          
          # Validate sub-200ms requirement
          max_allowed_ms = float('${{ env.PERFORMANCE_API_TIMEOUT }}')
          failed_tests = []
          
          for benchmark in results['benchmarks']:
              mean_time_ms = benchmark['stats']['mean'] * 1000
              test_name = benchmark['name']
              
              print(f'{test_name}: {mean_time_ms:.2f}ms (max allowed: {max_allowed_ms}ms)')
              
              if mean_time_ms > max_allowed_ms:
                  failed_tests.append(f'{test_name}: {mean_time_ms:.2f}ms')
          
          if failed_tests:
              print(f'ERROR: {len(failed_tests)} API tests exceeded {max_allowed_ms}ms requirement:')
              for test in failed_tests:
                  print(f'  - {test}')
              sys.exit(1)
          else:
              print(f'SUCCESS: All API tests meet sub-{max_allowed_ms}ms requirement')
          "

      # Database Query Performance Benchmarking per Section 4.7.1  
      - name: Database Query Performance Benchmarking
        run: |
          echo "Starting database query performance benchmarking..."
          
          # Run database performance benchmarks with sub-100ms validation
          python -m pytest tests/performance/test_database_benchmarks.py \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=${{ env.BENCHMARK_WARMUP_ITERATIONS }} \
            --benchmark-min-rounds=${{ env.BENCHMARK_MIN_ROUNDS }} \
            --benchmark-max-time=${{ env.BENCHMARK_MAX_TIME }} \
            --benchmark-json=database-benchmarks.json \
            --benchmark-histogram=database-query-histogram \
            --benchmark-save=database-benchmarks \
            --benchmark-save-data \
            -v --tb=short
          
          # Validate database response time requirements per Section 4.11.1
          python -c "
          import json
          import sys
          
          # Load benchmark results
          with open('database-benchmarks.json', 'r') as f:
              results = json.load(f)
          
          # Validate sub-100ms requirement
          max_allowed_ms = float('${{ env.PERFORMANCE_DB_TIMEOUT }}')
          failed_tests = []
          
          for benchmark in results['benchmarks']:
              mean_time_ms = benchmark['stats']['mean'] * 1000
              test_name = benchmark['name']
              
              print(f'{test_name}: {mean_time_ms:.2f}ms (max allowed: {max_allowed_ms}ms)')
              
              if mean_time_ms > max_allowed_ms:
                  failed_tests.append(f'{test_name}: {mean_time_ms:.2f}ms')
          
          if failed_tests:
              print(f'ERROR: {len(failed_tests)} database tests exceeded {max_allowed_ms}ms requirement:')
              for test in failed_tests:
                  print(f'  - {test}')
              sys.exit(1)
          else:
              print(f'SUCCESS: All database tests meet sub-{max_allowed_ms}ms requirement')
          "

      # Authentication Performance Benchmarking per Section 4.11.1
      - name: Authentication Performance Benchmarking  
        run: |
          echo "Starting authentication performance benchmarking..."
          
          # Run authentication performance benchmarks with sub-150ms validation
          python -m pytest tests/performance/test_authentication_benchmarks.py \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=${{ env.BENCHMARK_WARMUP_ITERATIONS }} \
            --benchmark-min-rounds=${{ env.BENCHMARK_MIN_ROUNDS }} \
            --benchmark-max-time=${{ env.BENCHMARK_MAX_TIME }} \
            --benchmark-json=auth-benchmarks.json \
            --benchmark-histogram=auth-response-histogram \
            --benchmark-save=auth-benchmarks \
            --benchmark-save-data \
            -v --tb=short
          
          # Validate authentication response time requirements per Section 4.11.1
          python -c "
          import json
          import sys
          
          # Load benchmark results
          with open('auth-benchmarks.json', 'r') as f:
              results = json.load(f)
          
          # Validate sub-150ms requirement
          max_allowed_ms = float('${{ env.PERFORMANCE_AUTH_TIMEOUT }}')
          failed_tests = []
          
          for benchmark in results['benchmarks']:
              mean_time_ms = benchmark['stats']['mean'] * 1000
              test_name = benchmark['name']
              
              print(f'{test_name}: {mean_time_ms:.2f}ms (max allowed: {max_allowed_ms}ms)')
              
              if mean_time_ms > max_allowed_ms:
                  failed_tests.append(f'{test_name}: {mean_time_ms:.2f}ms')
          
          if failed_tests:
              print(f'ERROR: {len(failed_tests)} auth tests exceeded {max_allowed_ms}ms requirement:')
              for test in failed_tests:
                  print(f'  - {test}')
              sys.exit(1)
          else:
              print(f'SUCCESS: All auth tests meet sub-{max_allowed_ms}ms requirement')
          "

      # Memory Usage Profiling per Section 6.5.1.1
      - name: Memory Usage Profiling and Analysis
        if: env.MEMORY_PROFILING_ENABLED == 'true' || github.event.inputs.memory_profiling == 'true'
        run: |
          echo "Starting comprehensive memory profiling analysis..."
          
          # Run memory profiling benchmarks
          python -m pytest tests/performance/test_memory_profiling.py \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=${{ env.BENCHMARK_WARMUP_ITERATIONS }} \
            --benchmark-min-rounds=${{ env.BENCHMARK_MIN_ROUNDS }} \
            --benchmark-max-time=${{ env.BENCHMARK_MAX_TIME }} \
            --benchmark-json=memory-benchmarks.json \
            --benchmark-histogram=memory-usage-histogram \
            --benchmark-save=memory-benchmarks \
            --benchmark-save-data \
            -v --tb=short
          
          # Generate detailed memory profiling report
          python -c "
          import json
          import psutil
          import gc
          
          print('=== Memory Profiling Analysis ===')
          print(f'System Memory: {psutil.virtual_memory()}')
          print(f'System Swap: {psutil.swap_memory()}')
          print(f'Python GC Stats: {gc.get_stats()}')
          print()
          
          # Load memory benchmark results
          with open('memory-benchmarks.json', 'r') as f:
              results = json.load(f)
          
          print('=== Memory Benchmark Results ===')
          for benchmark in results['benchmarks']:
              name = benchmark['name']
              stats = benchmark['stats']
              print(f'{name}:')
              print(f'  Mean execution time: {stats[\"mean\"]*1000:.2f}ms')
              print(f'  Memory overhead: Measured via memory_profiler')
              print()
          "
          
          # Create memory profiling visualization
          python -c "
          import matplotlib.pyplot as plt
          import json
          
          # Load benchmark data
          with open('memory-benchmarks.json', 'r') as f:
              data = json.load(f)
          
          # Create memory usage visualization
          test_names = [b['name'] for b in data['benchmarks']]
          mean_times = [b['stats']['mean']*1000 for b in data['benchmarks']]
          
          plt.figure(figsize=(12, 6))
          plt.bar(range(len(test_names)), mean_times)
          plt.xlabel('Memory Tests')
          plt.ylabel('Execution Time (ms)')
          plt.title('Memory Profiling Performance Results')
          plt.xticks(range(len(test_names)), test_names, rotation=45, ha='right')
          plt.tight_layout()
          plt.savefig('memory-profiling-chart.png', dpi=300, bbox_inches='tight')
          print('Memory profiling visualization saved to memory-profiling-chart.png')
          "

      # Concurrent Load Testing per Section 8.4.3
      - name: Concurrent User Load Testing
        run: |
          echo "Starting concurrent user load testing validation..."
          
          # Run concurrent load testing benchmarks
          python -m pytest tests/performance/test_concurrent_load.py \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=${{ env.BENCHMARK_WARMUP_ITERATIONS }} \
            --benchmark-min-rounds=${{ env.BENCHMARK_MIN_ROUNDS }} \
            --benchmark-max-time=${{ env.BENCHMARK_MAX_TIME }} \
            --benchmark-json=concurrent-benchmarks.json \
            --benchmark-histogram=concurrent-load-histogram \
            --benchmark-save=concurrent-benchmarks \
            --benchmark-save-data \
            -v --tb=short
          
          # Analyze concurrent load handling performance
          python -c "
          import json
          import sys
          
          # Load concurrent load benchmark results
          with open('concurrent-benchmarks.json', 'r') as f:
              results = json.load(f)
          
          print('=== Concurrent Load Testing Results ===')
          
          performance_issues = []
          
          for benchmark in results['benchmarks']:
              name = benchmark['name']
              stats = benchmark['stats']
              mean_time_ms = stats['mean'] * 1000
              throughput_ops = stats.get('ops', 0)
              
              print(f'{name}:')
              print(f'  Mean response time: {mean_time_ms:.2f}ms')
              print(f'  Throughput: {throughput_ops:.2f} ops/sec')
              print(f'  Standard deviation: {stats[\"stddev\"]*1000:.2f}ms')
              
              # Check for performance degradation under load
              if 'concurrent' in name.lower() and mean_time_ms > 500:
                  performance_issues.append(f'{name}: {mean_time_ms:.2f}ms response time under load')
              
              print()
          
          if performance_issues:
              print('WARNING: Performance degradation detected under concurrent load:')
              for issue in performance_issues:
                  print(f'  - {issue}')
          else:
              print('SUCCESS: System maintains performance under concurrent load')
          "

      # Node.js Baseline Comparison per Section 4.7.2
      - name: Baseline Comparison Against Node.js System
        if: github.event.inputs.baseline_comparison == 'true' || github.event_name == 'schedule'
        continue-on-error: true  # Allow workflow to continue if Node.js system unavailable
        run: |
          echo "Starting baseline comparison against Node.js system..."
          
          # Run baseline comparison tests
          python -m pytest tests/performance/test_baseline_comparison.py \
            --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=${{ env.BENCHMARK_WARMUP_ITERATIONS }} \
            --benchmark-min-rounds=${{ env.BENCHMARK_MIN_ROUNDS }} \
            --benchmark-max-time=${{ env.BENCHMARK_MAX_TIME }} \
            --benchmark-json=baseline-comparison.json \
            --benchmark-histogram=baseline-comparison-histogram \
            --benchmark-save=baseline-comparison \
            --benchmark-save-data \
            -v --tb=short || echo "Baseline comparison completed with warnings"
          
          # Analyze baseline comparison results
          if [ -f baseline-comparison.json ]; then
            python -c "
            import json
            import sys
            
            # Load baseline comparison results
            with open('baseline-comparison.json', 'r') as f:
                results = json.load(f)
            
            print('=== Baseline Comparison Analysis ===')
            
            parity_violations = []
            performance_improvements = []
            
            for benchmark in results['benchmarks']:
                name = benchmark['name']
                stats = benchmark['stats']
                mean_time_ms = stats['mean'] * 1000
                
                # Analyze performance relative to baseline
                if 'flask_vs_nodejs' in name.lower():
                    print(f'{name}: {mean_time_ms:.2f}ms')
                    
                    # Check for significant performance differences
                    # (This would compare against stored Node.js metrics)
                    if mean_time_ms < 100:  # Performance improvement
                        performance_improvements.append(name)
                    elif mean_time_ms > 300:  # Performance degradation
                        parity_violations.append(name)
            
            print()
            if performance_improvements:
                print('Performance improvements detected:')
                for improvement in performance_improvements:
                    print(f'  âœ“ {improvement}')
                print()
            
            if parity_violations:
                print('Performance parity violations detected:')
                for violation in parity_violations:
                    print(f'  âš  {violation}')
                print()
            
            print('Baseline comparison analysis complete')
            "
          fi

      # Performance Regression Detection per Section 8.4.3
      - name: Performance Regression Detection and Analysis
        run: |
          echo "Starting performance regression detection and analysis..."
          
          # Create comprehensive performance regression analysis
          python -c "
          import json
          import os
          import sys
          from datetime import datetime
          import statistics
          
          # Performance thresholds per Section 4.11.1
          THRESHOLDS = {
              'api_response_ms': float('${{ env.PERFORMANCE_API_TIMEOUT }}'),
              'database_query_ms': float('${{ env.PERFORMANCE_DB_TIMEOUT }}'),  
              'auth_response_ms': float('${{ env.PERFORMANCE_AUTH_TIMEOUT }}'),
              'memory_overhead_mb': 512,  # Maximum memory overhead
              'concurrent_degradation_factor': 1.5  # Max acceptable degradation under load
          }
          
          # Load all benchmark results
          benchmark_files = [
              'api-benchmarks.json',
              'database-benchmarks.json', 
              'auth-benchmarks.json',
              'memory-benchmarks.json',
              'concurrent-benchmarks.json'
          ]
          
          all_results = {}
          performance_violations = []
          performance_summary = {
              'total_tests': 0,
              'passed_tests': 0,
              'failed_tests': 0,
              'warnings': 0
          }
          
          print('=== Performance Regression Analysis ===')
          print(f'Analysis timestamp: {datetime.utcnow().isoformat()}Z')
          print()
          
          # Analyze each benchmark file
          for benchmark_file in benchmark_files:
              if os.path.exists(benchmark_file):
                  with open(benchmark_file, 'r') as f:
                      results = json.load(f)
                  
                  category = benchmark_file.replace('-benchmarks.json', '')
                  all_results[category] = results
                  
                  print(f'=== {category.upper()} Performance Analysis ===')
                  
                  for benchmark in results['benchmarks']:
                      name = benchmark['name']
                      stats = benchmark['stats']
                      mean_time_ms = stats['mean'] * 1000
                      
                      performance_summary['total_tests'] += 1
                      
                      # Determine appropriate threshold
                      if 'api' in category:
                          threshold = THRESHOLDS['api_response_ms']
                      elif 'database' in category:
                          threshold = THRESHOLDS['database_query_ms']
                      elif 'auth' in category:
                          threshold = THRESHOLDS['auth_response_ms']
                      else:
                          threshold = 1000  # Default threshold
                      
                      # Check performance against threshold
                      status = 'âœ“ PASS' if mean_time_ms <= threshold else 'âœ— FAIL'
                      if mean_time_ms <= threshold:
                          performance_summary['passed_tests'] += 1
                      else:
                          performance_summary['failed_tests'] += 1
                          performance_violations.append({
                              'test': name,
                              'category': category,
                              'actual_ms': mean_time_ms,
                              'threshold_ms': threshold,
                              'violation_factor': mean_time_ms / threshold
                          })
                      
                      print(f'  {status} {name}: {mean_time_ms:.2f}ms (threshold: {threshold}ms)')
                  
                  print()
          
          # Performance Summary Report
          print('=== Performance Summary Report ===')
          print(f'Total Tests: {performance_summary[\"total_tests\"]}')
          print(f'Passed: {performance_summary[\"passed_tests\"]}')
          print(f'Failed: {performance_summary[\"failed_tests\"]}')
          success_rate = (performance_summary[\"passed_tests\"] / performance_summary[\"total_tests\"]) * 100 if performance_summary[\"total_tests\"] > 0 else 0
          print(f'Success Rate: {success_rate:.1f}%')
          print()
          
          # Violation Analysis
          if performance_violations:
              print('=== Performance Violations Detected ===')
              performance_violations.sort(key=lambda x: x['violation_factor'], reverse=True)
              
              for violation in performance_violations:
                  factor = violation['violation_factor']
                  print(f'  âš  {violation[\"test\"]} ({violation[\"category\"]})')
                  print(f'    Actual: {violation[\"actual_ms\"]:.2f}ms')
                  print(f'    Threshold: {violation[\"threshold_ms\"]:.2f}ms') 
                  print(f'    Violation Factor: {factor:.2f}x')
                  print()
              
              print(f'CRITICAL: {len(performance_violations)} performance regression(s) detected!')
              
              # Exit with error if critical violations found
              critical_violations = [v for v in performance_violations if v['violation_factor'] > 2.0]
              if critical_violations:
                  print(f'CRITICAL: {len(critical_violations)} severe performance violations (>2x threshold)')
                  sys.exit(1)
              else:
                  print('WARNING: Performance violations detected but within acceptable range')
          else:
              print('SUCCESS: No performance regressions detected')
              print('All tests meet performance requirements')
          
          # Save performance report
          report = {
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'summary': performance_summary,
              'violations': performance_violations,
              'thresholds': THRESHOLDS,
              'success_rate': success_rate
          }
          
          with open('performance-regression-report.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          print()
          print('Performance regression analysis complete')
          print('Report saved to performance-regression-report.json')
          "

      # Performance Trend Analysis and Historical Comparison
      - name: Performance Trend Analysis
        if: github.event_name == 'schedule' || github.event_name == 'push'
        run: |
          echo "Generating performance trend analysis..."
          
          # Create performance trend visualization
          python -c "
          import json
          import matplotlib.pyplot as plt
          import matplotlib.dates as mdates
          from datetime import datetime, timedelta
          import numpy as np
          
          # Load current performance data
          benchmark_files = [
              'api-benchmarks.json',
              'database-benchmarks.json',
              'auth-benchmarks.json'
          ]
          
          # Generate trend analysis chart
          fig, axes = plt.subplots(3, 1, figsize=(15, 12))
          fig.suptitle('Performance Trend Analysis - Flask Migration', fontsize=16, fontweight='bold')
          
          for idx, benchmark_file in enumerate(benchmark_files):
              if os.path.exists(benchmark_file):
                  with open(benchmark_file, 'r') as f:
                      results = json.load(f)
                  
                  category = benchmark_file.replace('-benchmarks.json', '').upper()
                  ax = axes[idx]
                  
                  # Extract test names and mean times
                  test_names = [b['name'] for b in results['benchmarks']]
                  mean_times = [b['stats']['mean'] * 1000 for b in results['benchmarks']]
                  
                  # Create bar chart for current performance
                  bars = ax.bar(range(len(test_names)), mean_times, alpha=0.7)
                  ax.set_title(f'{category} Performance Results', fontweight='bold')
                  ax.set_ylabel('Response Time (ms)')
                  ax.set_xticks(range(len(test_names)))
                  ax.set_xticklabels(test_names, rotation=45, ha='right')
                  
                  # Add threshold line
                  if 'api' in category.lower():
                      threshold = float('${{ env.PERFORMANCE_API_TIMEOUT }}')
                  elif 'database' in category.lower():
                      threshold = float('${{ env.PERFORMANCE_DB_TIMEOUT }}')
                  elif 'auth' in category.lower():
                      threshold = float('${{ env.PERFORMANCE_AUTH_TIMEOUT }}')
                  else:
                      threshold = 200
                  
                  ax.axhline(y=threshold, color='red', linestyle='--', alpha=0.7, 
                            label=f'Threshold ({threshold}ms)')
                  ax.legend()
                  
                  # Color bars based on threshold
                  for bar, time in zip(bars, mean_times):
                      if time > threshold:
                          bar.set_color('red')
                      elif time > threshold * 0.8:
                          bar.set_color('orange')
                      else:
                          bar.set_color('green')
                  
                  ax.grid(True, alpha=0.3)
          
          plt.tight_layout()
          plt.savefig('performance-trend-analysis.png', dpi=300, bbox_inches='tight')
          print('Performance trend analysis chart saved to performance-trend-analysis.png')
          "

      # Generate Comprehensive Performance Report
      - name: Generate Performance Report
        run: |
          echo "Generating comprehensive performance report..."
          
          # Create detailed performance report
          python -c "
          import json
          import os
          from datetime import datetime
          
          # Load performance data
          report_data = {
              'generated_at': datetime.utcnow().isoformat() + 'Z',
              'workflow_run': '${{ github.run_number }}',
              'commit_sha': '${{ github.sha }}',
              'branch': '${{ github.ref_name }}',
              'trigger': '${{ github.event_name }}',
              'performance_requirements': {
                  'api_response_time_ms': '${{ env.PERFORMANCE_API_TIMEOUT }}',
                  'database_query_time_ms': '${{ env.PERFORMANCE_DB_TIMEOUT }}',
                  'authentication_time_ms': '${{ env.PERFORMANCE_AUTH_TIMEOUT }}'
              },
              'test_results': {}
          }
          
          # Load all benchmark results
          benchmark_files = [
              ('api', 'api-benchmarks.json'),
              ('database', 'database-benchmarks.json'),
              ('authentication', 'auth-benchmarks.json'),
              ('memory', 'memory-benchmarks.json'),
              ('concurrent', 'concurrent-benchmarks.json')
          ]
          
          for category, filename in benchmark_files:
              if os.path.exists(filename):
                  with open(filename, 'r') as f:
                      data = json.load(f)
                  
                  # Process benchmark data
                  test_results = []
                  for benchmark in data['benchmarks']:
                      test_result = {
                          'name': benchmark['name'],
                          'mean_time_ms': benchmark['stats']['mean'] * 1000,
                          'min_time_ms': benchmark['stats']['min'] * 1000,
                          'max_time_ms': benchmark['stats']['max'] * 1000,
                          'stddev_ms': benchmark['stats']['stddev'] * 1000,
                          'median_ms': benchmark['stats']['median'] * 1000,
                          'ops_per_second': benchmark['stats'].get('ops', 0),
                          'rounds': benchmark['stats']['rounds']
                      }
                      test_results.append(test_result)
                  
                  report_data['test_results'][category] = test_results
          
          # Load regression analysis if available
          if os.path.exists('performance-regression-report.json'):
              with open('performance-regression-report.json', 'r') as f:
                  regression_data = json.load(f)
              report_data['regression_analysis'] = regression_data
          
          # Save comprehensive report
          with open('comprehensive-performance-report.json', 'w') as f:
              json.dump(report_data, f, indent=2)
          
          print('Comprehensive performance report generated')
          print(f'Report includes {len(report_data[\"test_results\"])} test categories')
          "
          
          # Generate human-readable performance summary
          echo "=== PERFORMANCE TEST SUMMARY ===" > performance-summary.txt
          echo "Generated: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> performance-summary.txt
          echo "Workflow: ${{ github.workflow }} #${{ github.run_number }}" >> performance-summary.txt
          echo "Commit: ${{ github.sha }}" >> performance-summary.txt
          echo "Branch: ${{ github.ref_name }}" >> performance-summary.txt
          echo "" >> performance-summary.txt
          
          echo "=== PERFORMANCE REQUIREMENTS ===" >> performance-summary.txt
          echo "API Response Time: < ${{ env.PERFORMANCE_API_TIMEOUT }}ms" >> performance-summary.txt
          echo "Database Query Time: < ${{ env.PERFORMANCE_DB_TIMEOUT }}ms" >> performance-summary.txt
          echo "Authentication Time: < ${{ env.PERFORMANCE_AUTH_TIMEOUT }}ms" >> performance-summary.txt
          echo "" >> performance-summary.txt
          
          # Add test results summary
          if [ -f performance-regression-report.json ]; then
            python -c "
            import json
            with open('performance-regression-report.json', 'r') as f:
                data = json.load(f)
            
            print('=== TEST RESULTS SUMMARY ===')
            print(f'Total Tests: {data[\"summary\"][\"total_tests\"]}')
            print(f'Passed: {data[\"summary\"][\"passed_tests\"]}')
            print(f'Failed: {data[\"summary\"][\"failed_tests\"]}')
            print(f'Success Rate: {data[\"success_rate\"]:.1f}%')
            print()
            
            if data['violations']:
                print('=== PERFORMANCE VIOLATIONS ===')
                for violation in data['violations']:
                    print(f'{violation[\"test\"]}: {violation[\"actual_ms\"]:.2f}ms (>{violation[\"threshold_ms\"]}ms)')
            else:
                print('âœ“ All performance tests passed')
            " >> performance-summary.txt
          fi

      # Store Performance Artifacts
      - name: Upload Performance Artifacts
        uses: actions/upload-artifact@v4
        if: always()  # Upload artifacts even if tests fail
        with:
          name: performance-test-results-${{ github.run_number }}
          path: |
            *-benchmarks.json
            *-histogram.svg
            performance-*.png
            performance-*.txt
            performance-*.json
            system-baseline.log
            .benchmarks/
          retention-days: 30

      # Performance Alert Notifications
      - name: Performance Alert Notifications  
        if: failure()
        run: |
          echo "Performance test failures detected - sending alerts..."
          
          # Create performance alert message
          cat > performance-alert.json << EOF
          {
            "workflow": "${{ github.workflow }}",
            "run_number": "${{ github.run_number }}",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "triggered_by": "${{ github.event_name }}",
            "alert_level": "HIGH",
            "message": "Performance regression detected in Flask migration",
            "details": "One or more performance tests exceeded acceptable thresholds",
            "action_required": "Review performance test results and optimize implementation",
            "artifacts_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          }
          EOF
          
          echo "Performance alert notification prepared"
          echo "Alert details saved to performance-alert.json"
          
          # Display alert summary
          echo ""
          echo "ðŸš¨ PERFORMANCE ALERT ðŸš¨"
          echo "Performance regression detected!"
          echo "Review the uploaded artifacts for detailed analysis."
          echo "Workflow run: ${{ github.run_id }}"

      # Performance Success Notification
      - name: Performance Success Notification
        if: success()
        run: |
          echo "âœ… Performance tests completed successfully!"
          echo ""
          echo "=== PERFORMANCE VALIDATION SUCCESS ==="
          echo "All performance benchmarks passed requirements"
          echo "Flask implementation meets or exceeds Node.js baseline"
          echo "Migration performance validation: âœ… PASSED"
          echo ""
          echo "Detailed results available in uploaded artifacts"
          echo "Workflow run: ${{ github.run_id }}"

  # Performance Comparison Summary Job
  performance-summary:
    name: Performance Test Summary
    runs-on: ubuntu-24.04
    needs: performance-testing
    if: always()  # Run even if performance tests fail
    
    steps:
      - name: Download Performance Artifacts
        uses: actions/download-artifact@v4
        with:
          name: performance-test-results-${{ github.run_number }}
          path: ./performance-results

      - name: Generate Performance Summary
        run: |
          cd performance-results
          
          echo "=== PERFORMANCE TEST EXECUTION SUMMARY ===" 
          echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "Workflow: ${{ github.workflow }} #${{ github.run_number }}"
          echo "Repository: ${{ github.repository }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo ""
          
          # Count available result files
          json_files=$(find . -name "*-benchmarks.json" | wc -l)
          histogram_files=$(find . -name "*-histogram.svg" | wc -l)
          
          echo "=== ARTIFACTS GENERATED ==="
          echo "Benchmark JSON files: $json_files"
          echo "Performance histograms: $histogram_files"
          echo "Available files:"
          ls -la *.json *.png *.txt 2>/dev/null || echo "No additional files found"
          echo ""
          
          # Display performance summary if available
          if [ -f performance-summary.txt ]; then
            echo "=== PERFORMANCE SUMMARY ==="
            cat performance-summary.txt
          else
            echo "Performance summary not available"
          fi
          
          echo ""
          echo "Performance testing workflow completed"
          echo "Review artifacts for detailed performance analysis"
